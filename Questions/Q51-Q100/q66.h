#pragma once

#include <Header_Files/pch.h>
#include <Header_Files/uint128_t.h>
#include <Functions/continued_fractions.h>

//Diophantine Equation
std::pair<std::string, double> q66()
{
	auto run_time = std::chrono::steady_clock::now();

	int answer = 0, deepestRoute = 0;
	uint128_t Maximum = 0;
	for (int n = 2; n < 1001; n++)
	{
		int n_root = sqrt(n), currentRoute = 0;;
		if (n == (n_root * n_root)) continue; //skip numbers that are perfect squares

		uint128_t nums[2] = { 1, n_root }, dens[2] = { 0, 1 };
		bool location = false;
		long long triple[3] = { n_root, 0, 1 };

		while (true)
		{
			currentRoute++;
			if (currentRoute > deepestRoute) deepestRoute = currentRoute;

			//extend down to the next level of the repeated fraction
			triple[1] = triple[0] * triple[2] - triple[1];
			triple[2] = (n - (triple[1] * triple[1])) / triple[2];
			triple[0] = (n_root + triple[1]) / triple[2];

			//calculate the next convergent of the repeated fraction
			nums[location] += triple[0] * nums[!location];
			dens[location] += triple[0] * dens[!location];

			if ((nums[location] * nums[location] - n * dens[location] * dens[location]) == 1)
			{
				if (nums[location] > Maximum)
				{
					
					Maximum = nums[location];
					answer = n;
				}
				break;
			}

			//invert the variable keeping track of the fraction's location inside the num/den arrays
			location = !location;
		}
	}

	return { std::to_string(answer), std::chrono::duration_cast<std::chrono::nanoseconds>(std::chrono::steady_clock::now() - run_time).count() / 1000000000.0 };

	//the answer is 661
	//ran in 0.0016516 seconds
}

//NOTES
//I know from experience that working with Diophantine equations is a pain in the ass so I elected to try and find some existing info on this particular one.
//A little research shows that x^2 - Dy^2 = 1 is what's known as Pell's equation and the base solution for any positive integer of D (other than when D is a 
//perfect square) can be generated by looking at succesive elements for the repeated fraction representation of sqrt(D). This seems fitting as the last few
//questions were also about repeating fractions. Essentially all that needs to be done here is tweak some of the repeated fraction code from the last few problems
//and use it to expand a repeated fraction until we find our first solution to x^2 - Dy^2 = 1. Do this for all non-square numbers less than 1000 and the answer
//should be found pretty quickly. The only snag I see with this approach is that each iteration of the repeated fraction needs to be calculated from the bottom up,
//so if hypothetically the first solution isn't obtained until the 100th iteration then 100 * 101 / 2 = 5,050 divisions would need to be done to find the base solution
//for a single number. If there was a better way to iteratively get the next fraction in a repeated fraction sequence (i.e. a top down approach) the code would be much
//more efficient

//The original code runs well enough, however, I know it would be a lot faster if I had an iterative approach to calculating the nth convergent for sqrt(n). Did
//a little reading on the Wikipedia page for continued fractions and found an iterative solution to find successive convergents for a given continued fraction of the form
//[a[0], a[1], a[2], a[3] ... a[n]]. The convergent (h[n] / k[n]) is given by: (a[n] * h[n-1] + h[k-2]) / (a[n] * k[n-1] + k[n-2]) which is actually a pretty straightforward
//formula. So all that's needed to do is to calculate the repeated a[n] represenatation in it's entirety and then just keep iterating over each element until the first
//solution to Pell's equation is found. Dissapointingly enough, after taking a decent amount of time to code this iterative implementation the program run time actually
//increased from 0.0758 seconds to 0.997 seconds. Since the iterative solution requires the entirety of the repeating sequence for the repeated fraction to be found, for 
//any number that has a very long sequence but has a very early convergent that solves the Diophantine equation, there are a lot of unnecessary calculations that happen.

//A third approach is to combine 1 and 2, i.e. use the iterative approach to finding each convergent but do it at each stage of the calculation for the next A in the overall
//repeated fraction sequence. If the answer to the Diophantine equation is found before the full repeating fraction sequence is found then great. If not then continue approach
//3 in the same manner is approach 2 and just keep cycling through the repeating digits iteratively until the answer is found. Annoyingly enough, this method ended up taking
//even more time than the second approach (by.008 seconds) It seems that enough of the Diophantine answers are small enough that an iterative approach isn't necessary and ends
//up slowing down the overall problem. In a perfect world I think the iterative approach should still be faster, however, since it requires the use of a map of vectors to known
//when the continued fraction is complete and uses the bigint class which inherently is slower than c++ built in types it just doesn't work out. I've decided to keep the original
//algorithm but have left version three commented in place just as a reference.

//--7/3/23 Update
/*
Going back over my notes here I had mentioned that I was going to use the convergent = (a[n] * h[n-1] + h[k-2]) / (a[n] * k[n-1] + k[n-2]) method in hopes of getting a 
speedup, but ultimately wasn't able to. Well I just went and coded this method up again and it's definitely quicker, so I'm not really sure what I did the first time around.
With that said, it's not as much faster as I would've thought. First, I upgraded the code to use int_64x instead of bigint which made the runtime drop from ~0.07 seconds to 
~0.04 seconds. Then implementing the quicker convergent method made the runtime drop again to ~0.01 seconds.

Reading the forum it seems that most fast solvers did so in the same manner here (or at least similar). I think what it boils down to is how good your large integer library
is. It seems that some people running the slow Python were still able to get the answer in a few milliseconds which I can only attribute to much quicker multiplication for 
numbers larger than 64-bits. The wikipedia page for Pell's equation mentions that an algorithm called the Schönhage–Strassen algorithm for fast integer multiplication can be 
used to calculate the fundamental solution in logarithmic time. It's been awhile but I think the multiplication technique I used for my int_64x class was based off of the Karatsuba 
technique so maybe this other technique would give me the speed up I'm looking for.
*/

//--9/26/24 Update
/*
A few months ago I made a 128-bit integer class. It's similar to the int64_x class that I made last year, however, since it's capped out at 128-bits a lot of the 
calculations are much simpler and faster. The largest number seen for this problem just fits into 128-bits so changing from int_64x to uint128_t types gets the same 
answer here but runs in 1/10 the runtime.
*/